{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network analysis algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression , Ridge , Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2 lists to separate the values from the index which correspond to edges\n",
    "edge1 = []\n",
    "edge2 = []\n",
    "for i in range(0,len(future_connections)):\n",
    "    edge1.append(future_connections.index[i][0])\n",
    "for i in range(0,len(future_connections)):\n",
    "    edge2.append(future_connections.index[i][1])\n",
    "    \n",
    "#Create columns with the values from the lists we just created  \n",
    "future_connections[\"edge1\"] = edge1\n",
    "future_connections[\"edge2\"] = edge2\n",
    "\n",
    "#Compute centrality measures for each edge and create columns with those values\n",
    "#Common neighbors centrality \n",
    "common_neighbors = []\n",
    "for i in range(0,len(future_connections)):\n",
    "    common_neighbors.append(len(list(nx.common_neighbors(G,future_connections[\"edge1\"][i],future_connections[\"edge2\"][i]))))\n",
    "future_connections[\"common_neighbors\"] = common_neighbors   \n",
    "\n",
    "#Jaccard coefficients centrality\n",
    "jaccard_coefficients = list(nx.jaccard_coefficient(G, list(future_connections.index))) \n",
    "jaccard_coefficient = []\n",
    "for i in range(0,len(jaccard_coefficients)):\n",
    "    jaccard_coefficient.append(jaccard_coefficients[i][2])   \n",
    "future_connections[\"jaccard_coefficients\"] = jaccard_coefficient  \n",
    "\n",
    "#Resource allocation index centrality\n",
    "resource_allocation_index = list(nx.resource_allocation_index(G, list(future_connections.index))) \n",
    "resource_allocation_indexes = []\n",
    "for i in range(0,len(resource_allocation_index)):\n",
    "    resource_allocation_indexes.append(resource_allocation_index[i][2])    \n",
    "future_connections[\"resource_allocation_index\"] = resource_allocation_indexes\n",
    "\n",
    "#Adamic adar index centrality\n",
    "adamic_adar_index = list(nx.adamic_adar_index(G, list(future_connections.index)))\n",
    "adamic_adar_indexes = []\n",
    "for i in range(0,len(adamic_adar_index)):\n",
    "    adamic_adar_indexes.append(adamic_adar_index[i][2])\n",
    "future_connections[\"adamic_adar_index\"] = adamic_adar_indexes\n",
    "\n",
    "#Preferential attachment centrality\n",
    "preferential_attachment = list(nx.preferential_attachment(G, list(future_connections.index)))\n",
    "preferential_attachments = []\n",
    "for i in range(0,len(adamic_adar_index)):\n",
    "    preferential_attachments.append(preferential_attachment[i][2])\n",
    "future_connections[\"preferential_attachment\"] = preferential_attachments\n",
    "\n",
    "#Drop the columns we created with the nodes values\n",
    "future_connections.drop(\"edge1\", axis = 1, inplace =True)\n",
    "future_connections.drop(\"edge2\", axis = 1, inplace =True)\n",
    "\n",
    "#Create set we will predict \n",
    "to_be_predicted = future_connections[future_connections[\"Future Connection\"].isnull()]\n",
    "to_be_predicted = to_be_predicted.iloc[:,1:]\n",
    "\n",
    "#Create data\n",
    "complete_data = future_connections[future_connections[\"Future Connection\"].notnull()]\n",
    "data = complete_data.iloc[:,1:]\n",
    "\n",
    "#Create target\n",
    "target = complete_data[\"Future Connection\"]\n",
    "\n",
    "#Train test split\n",
    "X_train , X_test , y_train , y_test = train_test_split(data , target)\n",
    "\n",
    "#Rodge regression model\n",
    "clf = Ridge()\n",
    "clf.fit(X_train , y_train)\n",
    "\n",
    "#Predict x_test values\n",
    "y_scores = clf.predict(X_test)\n",
    "\n",
    "#Compute auc score\n",
    "score = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "#Predic final set\n",
    "prediction = clf.predict(to_be_predicted)\n",
    "\n",
    "#Return Series with the prediction probabilities and its index\n",
    "serie = pd.Series(prediction ,to_be_predicted.index )\n",
    "Serie\n",
    "\n",
    "Print(\"Model with 0.91 accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression , Ridge , Lasso\n",
    "\n",
    "\n",
    "#Create a dataframe using networkx Graph nodes.\n",
    "df = pd.DataFrame(index=G.nodes())\n",
    "\n",
    "#Then create new columns which use node attributes and centrality measures data\n",
    "df['Department'] = pd.Series(nx.get_node_attributes(G, 'Department'))\n",
    "df['ManagementSalary'] = pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))\n",
    "df[\"Clustering\"] = pd.Series(nx.clustering(G))\n",
    "df[\"Degree\"] = pd.Series(nx.degree(G))\n",
    "df[\"closeness_centrality\"] = pd.Series(nx.closeness_centrality(G))\n",
    "df[\"betweenness_centrality\"] = pd.Series(nx.betweenness_centrality(G))\n",
    "df[\"degree_centrality\"] = pd.Series(nx.degree_centrality(G))\n",
    "df[\"page_rank\"] = pd.Series(nx.pagerank(G))\n",
    "\n",
    "#We sort the df using the target values column in order to organize it\n",
    "df = df.sort_values(\"ManagementSalary\")\n",
    "\n",
    "#We create the data and target variables \n",
    "data = df.loc[:, df.columns != 'ManagementSalary']\n",
    "target = df.loc[:,\"ManagementSalary\"]\n",
    "    \n",
    "#We use slicing to create the variable set which we should predict later\n",
    "to_be_predicted = data[753:]\n",
    "data = data[:753]\n",
    "target = target[:753]\n",
    "\n",
    "#tran_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(data , target)\n",
    "\n",
    "#And we create a ridge model fitting it with the X_train and y train data\n",
    "clf = Lasso()\n",
    "clf.fit(X_train , y_train)\n",
    "\n",
    "#We predict the test set from which we do have y_true values in order to get a AUC model score\n",
    "y_scores = clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "#prediction = clf.Predict_proba(to_be_predicted)\n",
    "prediction = clf.predict(to_be_predicted)\n",
    "serie = pd.Series(prediction ,to_be_predicted.index )\n",
    "serie\n",
    "\n",
    "print(\"Model with <0.80 auc score accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch and LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read comma-separated files into DataFrames \n",
    "data= pd.read_csv(\"readonly/train.csv\",encoding = \"ISO-8859-1\")\n",
    "test_set = pd.read_csv(\"readonly/test.csv\", encoding = \"ISO-8859-1\")\n",
    "adresses_set = pd.read_csv(\"readonly/addresses.csv\", encoding = \"ISO-8859-1\")\n",
    "latlons_set = pd.read_csv(\"readonly/latlons.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "#Eliminate \"Compliance\" column  nan values\n",
    "data = data[np.isfinite(data[\"compliance\"])]\n",
    "\n",
    "#Merge \"adresses_set\" and \"latlons_set\" on overlapping \"adress\" column \n",
    "merged_sets= pd.merge(left_on=\"address\", right_on=\"address\", left=adresses_set, right=latlons_set)\n",
    "\n",
    "#Merge \"data\" and the DataFrame above on \"ticket_id\"\n",
    "merged1= pd.merge(left_on=\"ticket_id\", right_on=\"ticket_id\", left=data, right= merged_sets)\n",
    "\n",
    "#Create the test DataFrame with needed values from \"adresses_set\" and \"latlons_set\"\n",
    "test_merged1= pd.merge(on=\"ticket_id\", left=test_set, right=merged_sets)\n",
    "\n",
    "#Set \"ticket_id\" as index for both DataFrames\n",
    "merged1.set_index(\"ticket_id\", inplace=True)\n",
    "test_merged1.set_index(\"ticket_id\", inplace=True)\n",
    "\n",
    "#We will use only values from United States\n",
    "merged2= merged1[merged1.country==\"USA\"]\n",
    "\n",
    "#Create dictionaries with the coulumns that we dont need\n",
    "drop = [\"payment_amount\",\"payment_date\",\"payment_status\",\"balance_due\",\"collection_status\",\"compliance_detail\",\n",
    "        \"agency_name\",\"violator_name\",\"violation_street_number\",\"violation_street_name\",\"violation_zip_code\",\n",
    "        \"mailing_address_str_number\",\"mailing_address_str_name\",\"city\",\"state\",\"zip_code\",\"non_us_str_code\",\n",
    "        \"country\",\"ticket_issued_date\",\"hearing_date\",\"violation_description\",\"inspector_name\",\"address\",\n",
    "        \"grafitti_status\",\"violation_code\"]\n",
    "drop2=  [\"agency_name\",\"violator_name\",\"violation_street_number\",\"violation_street_name\",\"violation_zip_code\",\n",
    "        \"mailing_address_str_number\",\"mailing_address_str_name\",\"city\",\"state\",\"zip_code\",\"non_us_str_code\",\n",
    "         \"country\",\"ticket_issued_date\",\"hearing_date\",\"violation_description\",\"inspector_name\",\"address\",\n",
    "         \"grafitti_status\",\"violation_code\"]\n",
    "\n",
    "#Drop dictionaries\n",
    "merged2.drop(drop , axis=1, inplace=True)\n",
    "test_merged1.drop(drop2, axis= 1 , inplace=True)\n",
    "\n",
    "#Use Label enconder to enconde values as numbers\n",
    "label_encoder= LabelEncoder()\n",
    "label_encoder.fit(merged2['disposition'])\n",
    "merged2['disposition'] = label_encoder.transform(merged2['disposition'])\n",
    "\n",
    "label_encoder= LabelEncoder()\n",
    "label_encoder.fit(test_merged1['disposition'])\n",
    "test_merged1['disposition'] = label_encoder.transform(test_merged1['disposition'])\n",
    "\n",
    "#Set target\n",
    "data_target = merged2.loc[:, \"compliance\"]\n",
    "\n",
    "#Set data \n",
    "merged2.drop(\"compliance\", axis=1, inplace = True)\n",
    "data = merged2\n",
    "\n",
    "#Train_test_split\n",
    "X_train , X_test , y_train, y_test = train_test_split(data , data_target)\n",
    "\n",
    "#Fill nan values with the columns mean for both data sets\n",
    "X_train[\"lat\"]=X_train[\"lat\"].fillna(X_train[\"lat\"].mean())\n",
    "X_train[\"lon\"]=X_train[\"lon\"].fillna(X_train[\"lon\"].mean())\n",
    "X_test[\"lat\"]=X_test[\"lat\"].fillna(X_train[\"lat\"].mean())\n",
    "X_test[\"lon\"]=X_test[\"lon\"].fillna(X_train[\"lon\"].mean())\n",
    "\n",
    "test_merged1[\"lat\"]=test_merged1[\"lat\"].fillna(test_merged1[\"lat\"].mean())\n",
    "test_merged1[\"lon\"]=test_merged1[\"lon\"].fillna(test_merged1[\"lon\"].mean())\n",
    "\n",
    "#RandomForest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "#And we use GridSearch for selecting the best parameters\n",
    "grid_values = {'n_estimators' : [100,110], 'max_depth': [5,10]}\n",
    "grid_clf_auc = GridSearchCV(clf, param_grid=grid_values, scoring='roc_auc')\n",
    "grid_clf_auc.fit(X_train, y_train)\n",
    "\n",
    "#Return gri-auc score as a Series for the prediction set\n",
    "pd.Series(grid_clf_auc.predict_proba(test_merged1)[:,1], index=test_merged1.index)\n",
    "\n",
    "print(\"Model with 0.91 auc score accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
